\documentclass[12pt]{article}
\usepackage{setspace}
\doublespacing

\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{caption}
\usepackage{textcomp}    % for Hawaii characters

% shortcuts
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\blambda}{\boldsymbol{\lambda}}
\newcommand{\T}{\intercal}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\intd}{\mathrm{d}}
\newcommand{\bm}{\boldsymbol}  % bold maths symbols
\newcommand{\bs}{\boldsymbol{s}}
\newcommand{\tl}{\tilde{\lambda}}   % thinned little lambda
\newcommand{\tL}{\tilde{\Lambda}}  % thinned big lambda

% RJC 09/08/2019 Added shortcuts for Hawaiian words
\newcommand{\akepa}{\textquotesingle\={a}kepa}  % adds Hawaiian diacritical marks
\newcommand{\Akepa}{\textquotesingle\={A}kepa}  % adds Hawaiian diacritical marks
\newcommand{\hawaii}{Hawai\textquotesingle i}   % adds Hawaiian diacritical marks
\DeclareMathOperator*{\argmax}{arg\,max}  % * means _ puts thing beneath operator

\title{Extras}
\date{}

\begin{document}
	
\maketitle

These things might end up in supplementary materials and my thesis but probably not in any paper submission.

\section*{Estimating abundance}

In the main paper the posterior for abundance is estimated by taking posterior realisations of the intensity at every location within the study area.  This predicts the intensity within point transects in the same manner as outside the sampled region.  However, we could use the fact that we have observed counts within point transects.  If we denote the entire study region as $A$ and denote the random variable of abundance within a bounded subset $B \subset A$ as $N(B) \sim \text{Poisson}(\Lambda(B))$ where, $\Lambda(B) = \int_{\bs \in B} \lambda(\bs) \intd \boldsymbol{s}$.  Recall also that the entire surveyed region is denoted $\Omega$, the union of all point transects.  

The approach in the paper is to simulate intensities $\lambda^{(1)}, \ldots, \lambda^{(M)}$ over the entire study region $A$, where each $\lambda^{(m)}$ is sampled from the posterior distribution of $\lambda | \bm{Y}$, and then to approximate the posterior abundance via a Monte-carlo method as $N(A | \bm{Y}) \approx 1/M \sum_{m=1}^M N(A | \lambda = \lambda^{(m)}, \bm{Y})$.  However, an alternative approach is to apply the above method to only the un-sampled region $A \setminus \Omega$ and to predict only the \textit{undetected} counts within each transect.  The distribution of undetected counts across all transects is Poisson with rate parameter $ \tilde{\Lambda}(\Omega) := \int_{\bs \in \Omega}(1 - g(\bs))\lambda(\bs) \intd \bs$.  Using this approach the distribution of all undetected birds, whether through not being within the transects (and so in principle undetectable) or within a transect but not detected, is Poisson with rate parameter $\Lambda(A \setminus \Omega) + \tilde{\Lambda}(\Omega)$.  Then the distribution of total abundance can be derived by simply adding observed count to this probability distribution.  In theory, since this now uses the observed counts instead of predicting them, this should have lower variance compared to the \textit{predict everywhere} approach.  However, the point transects cover only $6.6\%$ of the total study area so the effect may be small.

To investigate whether this alternative approach makes a substantial difference to the results I compared the approximate posteriors using both methods defined using the same numerical integration scheme to ensure that any differences were not due to different schemes. Recall each point transect is denoted $\Omega_k$ with centroid $\bs_k$ and associated detection function $g_k(\bs)$. Then we can approximate the rate parameter for birds undetected within transects as 
\begin{align*}
 \tilde{\Lambda}(\Omega) &= \int_{\bs \in \Omega}(1 - g(\bs))\lambda(\bs) \intd \bs \\
 &= \sum_k \int_{\bs \in \Omega_k} (1 - g(\bs))\lambda(\bs) \intd \bs \\
 &\approx \sum_k 2 \pi \lambda(\bs_k) \int_0^W r(1-g(r))\intd r \\
 &\approx \sum_k \sum_j 2 \pi r_{kj} \alpha_{kj} \lambda(\bs_k)(1 - g(r_{kj}))
\end{align*}  
where locations $r_{kj}$ and weights $\alpha_{kj}$ are constructed following a mid-point integration scheme.  

The integration scheme for $\Lambda(A \setminus \Omega)$ is defined via projection of integration locations to mesh nodes, taking into account the fact that some mesh nodes may be within or near to point transects and so should have a lower weight than in comparison to the full mesh integration scheme.  This can be achieved within \texttt{inlabru} by using the \texttt{samplers} argument in the \texttt{ipoints()} function.  Figure \ref{fig:int_scheme_raw} shows the raw integration points for the integration scheme along side those for one that also integrates within transects for comparison.  These raw weights are then projected to mesh nodes which is shown in Figure \ref{fig:int_scheme_raw}
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{figures/compare_int_scheme_raw.png}
	\caption{Raw integration weights for integration over the whole study region (left) and integration over the whole region minus the transects (right).  The black circles are point transects.}
	\label{fig:int_scheme_raw}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{figures/compare_int_scheme_projected.png}
	\caption{Projected integration weights for integration over the whole study region (left) and integration over the whole region minus the transects (right).  The black circles are point transects.}
	\label{fig:int_scheme_projected}
\end{figure}
where one can see the lower weight at mesh nodes within or near point transects.  I use this integration scheme to integrate over $A \setminus \Omega$. For comparison I use this same integration scheme except do not thin within point transects to predict the abundance everywhere.  The posteriors estimated by both approaches are shown in Figure \ref{fig:compare_post_N}.
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{figures/compare_N_posteriors.png}
	\caption{comparing approximate posteriors.  Shaded regions are 2 monte-carlo standard errors above and below the mean}
	\label{fig:compare_post_N}
\end{figure}
This plot shows there is not a great deal of difference between the two approaches.  This may well be because the observed counts are so low compared to predicted abundance (153 compared to a range of 4000-8000 or so) and that detectability was fairly high.  Coupled with the fact that the sampled region is only $6.6\%$ of the total study region, it is perhaps not surprising that the results are almost identical.  I conclude that there is no much benefit to go through with the extra complication of using the observed counts over predicting everywhere.  However, for other populations this difference may well be large enough to be worth accounting for.  \texttt{inlabru} provides useful tools for this type of high-resolution quadrature that was necessary here given that transects were so small compared to the resolution of the mesh and the size of the study area as a whole.  

\section*{Understanding where we have gained information}

Given we have a posterior random field describing the density of birds, it is natural to ask questions along the lines of ``which areas of space have we learned more/less about the distribution of birds?".  Given that there is a correlation that decays with distance, we have clearly learned more than just the density within each point transect, but how can we say exactly where we have learned the most, given the locations of the point transects and the observations made there.  In this section I consider an approach to answering a question these types of questions by defining various metrics that seek to quantify this idea.  However, we will see that the resulting metrics are largely driven by the assumptions of the model and, although inspired by similar approaches in the geostatistical literature, they do not easily carry over into the Poisson process setting.  The basic intuition is to compare the posterior random field informed by the data $\lambda | \bm{Y}, \theta$ and the random field conditioned only on the posterior covariance structure $\lambda | \theta$, where $\theta$ is the hyper-parameters of the SPDE as well as the intercept variable.  The intercept should be included since the GRF and the intercept are not independent.  We consider two quantities
\begin{align}
   V_0(\bs) &= \mathbb{E}_{\theta | \bm{Y}} \left[ \text{Var}(\lambda(\bs) | \theta) \right] + \text{Var}_{\theta | \bm{Y}}\left[\mathbb{E}(\lambda(\bs) | \theta)\right] \\  
   V_{\bm{Y}}(\bs) &= \mathbb{E}_{\theta | \bm{Y}} \left[ \text{Var}(\lambda(\bs) | \theta, \bm{Y}) \right] + \text{Var}_{\theta | \bm{Y}}\left[\mathbb{E}(\lambda(\bs) | \theta, \bm{Y})\right].
\end{align}
Note that by the law of total variance $V_{\bm{Y}}(\bs) = \text{Var}(\lambda(\bs) | \bm{Y})$, the usual the usual posterior variance.  $V_0(\bs)$ is a adjusted version of these where the random field is considered only conditional on $\theta$ but not on the data directly.  This is a form of null model describing a random field with the same posterior covariance structure but no observations directly informing $\lambda$.  Note that $V_0(\bs) = V_{\bm{Y}}(\bs)$ and so by constructing metrics of the form
\begin{equation*}
		I_{\text{Var}}(\bs) = 1 - \frac{V_{\bm{Y}}(\bs)}{V_0(\bs)}
\end{equation*}  
we get a measure between 0 and 1 that describes how much the observations $\bm{Y}$ have informed the posterior random field at location $\bs$.  The subscript here indicates this is using the variance measure directly.  I also consider two alternatives:
\begin{align*}
	I_{\text{sd}}(\bs) &= 1 - \frac{\sqrt{V_{\bm{Y}}(\bs)}}{\sqrt{V_0(\bs)}} \\ \\
	I_{\text{cv}}(\bs) &= 1 - \frac{\sqrt{V_{\bm{Y}}(\bs)}/ \mu_{\bm{Y}}(\bs)}{\sqrt{V_0(\bs)}/\mu_0} \\
\end{align*}
where $\mu_{\bm{Y}}$ is the mean of $\lambda(\bs) | \bm{Y}, \theta$ and $\mu_0$ is the mean of $\lambda | \theta$.  These are the standard deviation and the coefficient of variation versions of the metric based on the variance.  The idea for considering all three is that the variance and standard deviation have a strong relationship with the mean due to the Poisson assumption. 

The usual posterior variance, standard deviation and coefficient of variation are all estimable by posterior sampling in \texttt{inlabru}.  I describe now a way to estimate $V_0(\bs)$ using Monte-carlo sampling based on the moments of the log-normal distribution.  This estimate can then be transformed as required for the $I_{\text{sd}}$ and $I_{\text{cv}}$ metrics.  We have $\log \lambda | \theta \sim N(m, \sigma^2)$ where the parameters $m$ and $\sigma^2$ are either included in or derivable from the parameter vector $\theta$.  Therefore,
\begin{align*}
   V_0(\bs) &= \mathbb{E}_{\theta | \bm{Y}} \left[ \text{Var}(\lambda(\bs) | \theta) \right] + \text{Var}_{\theta | \bm{Y}}\left[\mathbb{E}(\lambda(\bs) | \theta)\right] \\
   &= \mathbb{E}_{\theta | \bm{Y}} \left[ \exp(2m + \sigma^2)(\exp(\sigma^2) - 1) \right] + \text{Var}_{\theta | \bm{Y}}\left[ \exp(m + \sigma^2/2) \right]
\end{align*}
Given Monte-carlo samples $\theta^{(k)} \sim \theta | \bm{Y}$ for $k = 1, \ldots, K$ this can be estimated as
\begin{align*}
V_0(\bs) &\approx \frac{1}{K} \sum_k \exp(2m_k + \sigma^2_k)(\exp(\sigma_k^2) - 1) + \phantom{y} \\ 
&\phantom{\approx} \;\;\frac{1}{K} \sum_k \exp(2m_k + \sigma_k^2) - \frac{1}{K^2}\left[\sum_k \exp(m_k + \sigma_k^2 / 2) \right]^2 \\
&= \frac{1}{K} \sum_k \exp(2m_k + 2\sigma^2_k) - \frac{1}{K^2}\left[\sum_k \exp(m_k + \sigma_k^2 / 2) \right]^2
\end{align*}
The three metrics are shown in Figure \ref{fig:info.png}.
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{figures/info.png}
	\caption{Comparing different information metrics.  From left to right:  $I_{\text{Var}}$,  $I_{\text{sd}}$ and $I_{\text{cv}}$.}.
	\label{fig:info.png}
\end{figure}
The variance and standard deviation maps are dominated by the Poisson assumption.  Compare these to the posterior intensity and the look extremely similar.  The standard deviation metric looks to me more natural, in this example $V_0$ proved to be quite large (on order of 600,000) and so the metric is extremely close to 1 almost everywhere.  Taking the square root of this (about 775) provided a number more comparable with the scale of the variation in the posterior intensity.  Perhaps the most useful plot is $I_{\text{cv}}$.  This shows an intuitive map - in the north, where sampling effort is lower, we have less information than in the south, where sampling effort is higher.  One thing to note is that the posterior range of the random field is very large relative to the size of the study region.  For random fields where this range of correlation is lower, one might expect to see ``bumps" of higher information around each sampled location.  In this example the posterior field is too correlated for such an effect to be apparent.  It would be interested to try out these metrics in other settings as a useful way of communicating which regions of space we have the most information about.  The exact scale of each metric is hard to interpret, for now I am just looking at these differences in a relative sense (``more" or ``less" information compared to other regions).  

\section*{Spatio-temporal models}

Need to get other years of data from Rick to start looking into this.  

\end{document}
